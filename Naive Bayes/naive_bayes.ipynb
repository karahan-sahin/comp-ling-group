{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd06a346c8184832ade02c735d0547219f2b049e8e64c851ad09fbde8c70aea9d06",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spam\n.DS_Store\nlegitimate\n"
     ]
    }
   ],
   "source": [
    "# Import training data\n",
    "training_data = defaultdict(list)\n",
    "# conjoin spam text names with its current folder directory\n",
    "training_data[\"spam\"] = [os.path.join(f\"dataset/training/spam\",f) \n",
    "                         for f in os.listdir(f\"dataset/training/spam\")\n",
    "                         if f.endswith(\"txt\")]\n",
    "# conjoin legitimate text names with its current folder directory\n",
    "training_data[\"legitimate\"] = [os.path.join(f\"dataset/training/legitimate\",f) \n",
    "                               for f in os.listdir(f\"dataset/training/legitimate\")\n",
    "                               if f.endswith(\"txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = {\"id\": [],\n",
    "           \"vector\": [],\n",
    "           \"class\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parsing spam docs at the training data returns an count dictionary vector\n",
    "of spam mega document to used in NB algorithm.\n",
    "\"\"\"\n",
    "# Create an empty count dictionary for mega document\n",
    "spam_megadoc = defaultdict(int)\n",
    "# Iterate over spam documents\n",
    "for spam in training_data[\"spam\"]:\n",
    "    # Open the document\n",
    "    with open(spam, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        # Read the document as a string\n",
    "        data = f_in.read()\n",
    "        # Split the document into tokens\n",
    "        tokens = re.split(r'\\s|\\\\n',data)\n",
    "        # For each token append its frequency in mega document\n",
    "        text = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            text[token] += 1\n",
    "            spam_megadoc[token] += 1\n",
    "        \n",
    "        dataset[\"id\"].append(spam)\n",
    "        dataset[\"vector\"].append(text)\n",
    "        dataset[\"class\"].append(1.0)\n",
    "\n",
    "# Calculate the total number of individual tokens in the spam megadocument\n",
    "spam_token_size = 0\n",
    "for freq in spam_megadoc.values():\n",
    "    spam_token_size += freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subject: cable decsrambler $ 6 . 99 !\n\nreally cool ! premium channels pay per view events * * * * free * * * * * * test throughout europe ! * * easy assemble plan $ 6 . 99 usd ! send plan day receive order ! watching hbo , showtime , movie channel , pay per view event , adult station , scramble signal next week ! easily assemble cable descrambler less 30 minute ! probably many advertisment similar plan . . . . ours better ! compare actually improved quality simplified design ! ! ! * * even include photos ! * * our plans are better ! , easy read , easy assemble plan $ 6 . 99 usd ! advertise much $ 49 . 0 usd wait week receive ! others is true ! part available local electronics store ! call ask before order ! does indeed work ! need part # 's 270-235 271-1325 278-212 rg59 coaxial cable , # 12 copper wire , variable capacitor . * * part describe name instruction . * * * * special order variable capacitor . . . . why wait special order ? ! ! ! * * secure supply capacitor directly manufacturer , include one plan additional $ 10 . 0 usd ! legal , provide course plan educational purpose . ' s fun ! ' re sure ' ll enjoy ! our families sure ! * need one descrambler each tv . monthly bills ! $ 6 . 99 usd plan $ 10 . 0 usd variable capacitor $ 16 . 99 usd easy assemble plan one variable capacitor ! $ 1 . 50 usd ship handle internatinal orders ! pay check money order payable : hobby pro 336 bon air center # 254 greenbra , ca . 94904 usa please provide self address envelope priority delivery ! pay postage ! rush order ! int 1 * * * * *\n\n[defaultdict(<class 'int'>, {'Subject:': 1, 'cable': 3, 'decsrambler': 1, '$': 9, '6': 4, '.': 22, '99': 5, '!': 32, '': 2, 'really': 1, 'cool': 1, 'premium': 1, 'channels': 1, 'pay': 4, 'per': 2, 'view': 2, 'events': 1, '*': 30, 'free': 1, 'test': 1, 'throughout': 1, 'europe': 1, 'easy': 4, 'assemble': 4, 'plan': 8, 'usd': 8, 'send': 1, 'day': 1, 'receive': 2, 'order': 6, 'watching': 1, 'hbo': 1, ',': 12, 'showtime': 1, 'movie': 1, 'channel': 1, 'event': 1, 'adult': 1, 'station': 1, 'scramble': 1, 'signal': 1, 'next': 1, 'week': 2, 'easily': 1, 'descrambler': 2, 'less': 1, '30': 1, 'minute': 1, 'probably': 1, 'many': 1, 'advertisment': 1, 'similar': 1, 'ours': 1, 'better': 2, 'compare': 1, 'actually': 1, 'improved': 1, 'quality': 1, 'simplified': 1, 'design': 1, 'even': 1, 'include': 2, 'photos': 1, 'our': 2, 'plans': 1, 'are': 1, 'read': 1, 'advertise': 1, 'much': 1, '49': 1, '0': 3, 'wait': 2, 'others': 1, 'is': 1, 'true': 1, 'part': 3, 'available': 1, 'local': 1, 'electronics': 1, 'store': 1, 'call': 1, 'ask': 1, 'before': 1, 'does': 1, 'indeed': 1, 'work': 1, 'need': 2, '#': 3, \"'s\": 1, '270-235': 1, '271-1325': 1, '278-212': 1, 'rg59': 1, 'coaxial': 1, '12': 1, 'copper': 1, 'wire': 1, 'variable': 4, 'capacitor': 5, 'describe': 1, 'name': 1, 'instruction': 1, 'special': 2, 'why': 1, '?': 1, 'secure': 1, 'supply': 1, 'directly': 1, 'manufacturer': 1, 'one': 3, 'additional': 1, '10': 2, 'legal': 1, 'provide': 2, 'course': 1, 'educational': 1, 'purpose': 1, \"'\": 3, 's': 1, 'fun': 1, 're': 1, 'sure': 2, 'll': 1, 'enjoy': 1, 'families': 1, 'each': 1, 'tv': 1, 'monthly': 1, 'bills': 1, '16': 1, '1': 2, '50': 1, 'ship': 1, 'handle': 1, 'internatinal': 1, 'orders': 1, 'check': 1, 'money': 1, 'payable': 1, ':': 1, 'hobby': 1, 'pro': 1, '336': 1, 'bon': 1, 'air': 1, 'center': 1, '254': 1, 'greenbra': 1, 'ca': 1, '94904': 1, 'usa': 1, 'please': 1, 'self': 1, 'address': 1, 'envelope': 1, 'priority': 1, 'delivery': 1, 'postage': 1, 'rush': 1, 'int': 1})]\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/training/spam/spmsga65.txt\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "    d = f_in.read()\n",
    "print(d)\n",
    "print(np.asarray(df.iloc[[0]][\"vector\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parsing legitimate docs at the training data returns an count dictionary vector\n",
    "of legitimate mega document to used in NB algorithm.\n",
    "\"\"\"\n",
    "# Create an empty count dictionary for mega document\n",
    "legitimate_megadoc = defaultdict(int)\n",
    "# Iterate over Legitimate documents\n",
    "for spam in training_data[\"legitimate\"]:\n",
    "    # Open the document\n",
    "    with open(spam, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        # Read the document as a string\n",
    "        data = f_in.read()\n",
    "        # Split the document into tokens\n",
    "        tokens = re.split(r'\\s|\\\\n',data)\n",
    "        # For each token append its frequency in mega document\n",
    "        text = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            text[token] += 1\n",
    "            legitimate_megadoc[token] += 1\n",
    "        \n",
    "        dataset[\"id\"].append(spam)\n",
    "        dataset[\"vector\"].append(text)\n",
    "        dataset[\"class\"].append(0.0)\n",
    "\n",
    "# Calculate the total number of individual tokens in the Legitimate mega document\n",
    "legitimate_token_size = 0\n",
    "for freq in legitimate_megadoc.values():\n",
    "    legitimate_token_size += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 480 entries, 0 to 479\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   id      480 non-null    object \n 1   vector  480 non-null    object \n 2   class   480 non-null    float64\ndtypes: float64(1), object(2)\nmemory usage: 11.4+ KB\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             id  \\\n",
       "475  dataset/training/legitimate/5-1266msg3.txt   \n",
       "476  dataset/training/legitimate/5-1296msg2.txt   \n",
       "477   dataset/training/legitimate/3-384msg1.txt   \n",
       "478  dataset/training/legitimate/5-1291msg2.txt   \n",
       "479   dataset/training/legitimate/3-425msg1.txt   \n",
       "\n",
       "                                                vector  class  \n",
       "475  {'Subject:': 1, 'ucla': 6, 'tesl': 8, '&': 3, ...    0.0  \n",
       "476  {'Subject:': 1, 'translator': 8, '': 2, 'order...    0.0  \n",
       "477  {'Subject:': 1, 'next': 2, '': 2, 'message': 1...    0.0  \n",
       "478  {'Subject:': 1, 're': 2, ':': 4, '5': 2, '.': ...    0.0  \n",
       "479  {'Subject:': 1, 'language': 2, '?': 1, '': 2, ...    0.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>vector</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>475</th>\n      <td>dataset/training/legitimate/5-1266msg3.txt</td>\n      <td>{'Subject:': 1, 'ucla': 6, 'tesl': 8, '&amp;': 3, ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>476</th>\n      <td>dataset/training/legitimate/5-1296msg2.txt</td>\n      <td>{'Subject:': 1, 'translator': 8, '': 2, 'order...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>477</th>\n      <td>dataset/training/legitimate/3-384msg1.txt</td>\n      <td>{'Subject:': 1, 'next': 2, '': 2, 'message': 1...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>dataset/training/legitimate/5-1291msg2.txt</td>\n      <td>{'Subject:': 1, 're': 2, ':': 4, '5': 2, '.': ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>dataset/training/legitimate/3-425msg1.txt</td>\n      <td>{'Subject:': 1, 'language': 2, '?': 1, '': 2, ...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df.info()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes_Classifier(test_doc, alpha=0.001):\n",
    "    \"\"\"\n",
    "    Spam Classifier that Naivie Bayes Algorithm which is trained over 480 e-mails\n",
    "    either classified as spam or legitimate. The function takes a test document path\n",
    "    and returns a tuple with document path and classification tag \"spam: 1; legitimate: 0\"\n",
    "\n",
    "    In this version, the algorithm uses all the avaible vocabulary without any feature selection\n",
    "\n",
    "    -i: test document path: str\n",
    "    -o: (document path: str, classification tag: int)\n",
    "\n",
    "    \"\"\"\n",
    "    # All unique tokens in the training data\n",
    "    vocabulary_size = len(set(list(spam_megadoc.keys())+list(legitimate_megadoc.keys())))\n",
    "\n",
    "    # Prior Probabilities of Classes\n",
    "    prior_spam = len(training_data[\"spam\"])/len(training_data[\"spam\"] + training_data[\"legitimate\"])\n",
    "    prior_legitimate = len(training_data[\"legitimate\"])/len(training_data[\"spam\"] + training_data[\"legitimate\"])\n",
    "\n",
    "    # Open the test document\n",
    "    with open(test_doc, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        # Read the test document\n",
    "        data = f_in.read()\n",
    "\n",
    "    # Count dictionary for the test document\n",
    "    count_dict = defaultdict(int)\n",
    "\n",
    "    # Split data into tokens \n",
    "    tokens = re.split(r'\\s|\\\\n',data)\n",
    "\n",
    "    # Append term frequency for each token\n",
    "    for token in tokens:\n",
    "        count_dict[token] += 1\n",
    "\n",
    "    # Spam probability\n",
    "    sum_prob = 0\n",
    "    # Iterate over unique token\n",
    "    for word, freq in count_dict.items():\n",
    "        # Add the log probability of each token is in class spam   \n",
    "        sum_prob += math.log10((spam_megadoc[word]+alpha)/\n",
    "                                (spam_token_size+(alpha*vocabulary_size))) * freq\n",
    "\n",
    "    # Then add the log of prior probability of document to be in class spam\n",
    "    spam_prob = math.log10(prior_spam) + sum_prob  \n",
    "\n",
    "    # Legitimate probability\n",
    "    sum_prob = 0\n",
    "    # Iterate over unique token\n",
    "    for word, freq in count_dict.items():\n",
    "        # Add the log probability of each token is in class legitimate   \n",
    "        sum_prob += math.log10((legitimate_megadoc[word]+alpha)/\n",
    "                                (legitimate_token_size+(alpha*vocabulary_size))) * freq\n",
    "\n",
    "\n",
    "    # Then add the log of prior probability of document to be in class legitimate\n",
    "    legitimate_prob = math.log10(prior_legitimate) + sum_prob\n",
    "\n",
    "    # If it has higher probability in class spam\n",
    "    if spam_prob > legitimate_prob:\n",
    "        # Classify as spam\n",
    "        return (test_doc, 1)\n",
    "    # If not, it is \n",
    "    else:\n",
    "        # Classify as legitimate\n",
    "        return (test_doc, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary\n",
    "test_data = defaultdict(list)\n",
    "# conjoin spam text names with its current folder directory\n",
    "test_data[\"spam\"] = [os.path.join(\"dataset/test/spam\",f)\n",
    "                        for f in os.listdir(\"dataset/test/spam\")\n",
    "                        if f.endswith(\"txt\")]\n",
    "\n",
    "# conjoin legitimate text names with its current folder directory\n",
    "test_data[\"legitimate\"] = [os.path.join(\"dataset/test/legitimate\",f)\n",
    "                        for f in os.listdir(\"dataset/test/legitimate\")\n",
    "                        if f.endswith(\"txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_results = []\n",
    "# Run on Spam docs\n",
    "for tspam in test_data[\"spam\"]:\n",
    "    result = Naive_Bayes_Classifier(tspam)\n",
    "    classifier_results.append(result)\n",
    "\n",
    "# Run on Legitimate docs\n",
    "for tspam in test_data[\"legitimate\"]:\n",
    "    result = Naive_Bayes_Classifier(tspam)\n",
    "    classifier_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spam Classifier without Feature Selection:\n    Macro Precision: 0.9729248771680064\n    Macro Precision: 0.970954356846473\n    Macro Precision: 0.9748953974895398\n    Recall: 0.9729166666666667\n    F-measure: 0.9729207719000144\n    \n"
     ]
    }
   ],
   "source": [
    "#True Positive Spam\n",
    "tp_spam = 0\n",
    "#False Positive Legitimate\n",
    "fp_leg = 0\n",
    "\n",
    "#True Positive Legitimate\n",
    "tp_leg = 0\n",
    "#False Positive Spam\n",
    "fp_spam = 0\n",
    "\n",
    "# Iterate over classified docs\n",
    "for result in classifier_results:\n",
    "    # If the classified document is in spam docs\n",
    "    if result[0] in test_data[\"spam\"]:\n",
    "        # Check if it is classified correctly\n",
    "        if result[1] == 1:\n",
    "            # It is a True positive\n",
    "            tp_spam += 1\n",
    "        # If it is classified as legitimate document\n",
    "        # although it is a spam document\n",
    "        else:\n",
    "            # It is a False Positive\n",
    "            fp_leg += 1\n",
    "    \n",
    "    # If the classified document is in spam docs\n",
    "    elif result[0] in test_data[\"legitimate\"]:\n",
    "        # Check if it is classified correctly\n",
    "        if result[1] == 0:\n",
    "            # It is a True positive\n",
    "            tp_leg += 1\n",
    "        # If it is classified as legitimate document\n",
    "        # although it is a spam document\n",
    "        else:\n",
    "            # It is a False Positive\n",
    "            fp_spam += 1\n",
    "\n",
    "\n",
    "# If the classifier doesn't classify any document as spam\n",
    "try:\n",
    "    spam_precision = float(tp_spam / (tp_spam + fp_spam))\n",
    "except ZeroDivisionError:\n",
    "    spam_precision = 0\n",
    "\n",
    "# If the classifier doesn't classify any document as legitimate\n",
    "try:\n",
    "    legitimate_precision = float(tp_leg / (tp_leg + fp_leg))\n",
    "except ZeroDivisionError:\n",
    "    legitimate_precision = 0\n",
    "\n",
    "# Recall fraction of documents classified correctly out of all the documents that are classified\n",
    "recall = float((tp_spam + tp_leg) / (len(test_data[\"spam\"] + test_data[\"legitimate\"])))\n",
    "# Average of scores of precisions for each class \n",
    "macro_precision = (spam_precision + legitimate_precision) / 2\n",
    "# Calculate the F-measure score\n",
    "f_measure = (2*macro_precision*(recall)) / (macro_precision + (recall))\n",
    "\n",
    "print(f\"\"\"Spam Classifier without Feature Selection:\n",
    "    Macro Precision: {macro_precision}\n",
    "    Macro Precision: {spam_precision}\n",
    "    Macro Precision: {legitimate_precision}\n",
    "    Recall: {recall}\n",
    "    F-measure: {f_measure}\n",
    "    \"\"\")"
   ]
  }
 ]
}